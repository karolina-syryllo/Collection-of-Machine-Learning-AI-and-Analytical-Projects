
# Image caption generation project

    The aim of the project is to train the model to generate the relevant captions for a given image. Model is trained using a Flickr8K dataset. It contains a total of 8092 images in JPEG format with different shapes and sizes. Each image has assigned 5 different captions. 

    Specific objectives include:

    - displaying sample images along with their generated captions for both the RNN and LSTM version of the decoder,
    - comparing training using an RNN vs LSTM for the decoder network (loss, BLEU scores over test set, quality of generated captions, performance on long captions vs. short captions, etc.)








## 1.Present the sample images and generated caption for each epoch of training for both the RNN and LSTM version of the decoder, including the BLEU scores. 

###### LSTM MODEL

![IM1LSTM](https://user-images.githubusercontent.com/61549398/99193356-5fae2200-2770-11eb-97cc-a195b8a0b431.png)
![CAP1LSTM](https://user-images.githubusercontent.com/61549398/99193358-6046b880-2770-11eb-9074-44dfed328132.png)

![IM2LSTM](https://user-images.githubusercontent.com/61549398/99193485-275b1380-2771-11eb-80d8-ee2b7cb83830.png)
![CAP2LSTM](https://user-images.githubusercontent.com/61549398/99193486-288c4080-2771-11eb-8e36-ec52636b41c8.png)


Figure 1: Performance of LSTM during training on two images.
  
###### RNN MODEL

![IM1RNN](https://user-images.githubusercontent.com/61549398/99193657-5920aa00-2772-11eb-8c93-4bfbbf287a03.png)
![CAP1RNN](https://user-images.githubusercontent.com/61549398/99193661-5a51d700-2772-11eb-9e9e-09e4dcefd207.png)

![IM2RNN](https://user-images.githubusercontent.com/61549398/99193725-be749b00-2772-11eb-946a-0bed55bd9b52.png)
![CAP2RNN](https://user-images.githubusercontent.com/61549398/99193726-c0d6f500-2772-11eb-9b59-0444f2c31aa4.png)

Figure 2: Performance of RNN model during training on two images.

## 2.Comparing training using an RNN vs LSTM for the decoder network (loss, BLEU scores over test set, quality of generated captions, performance on long captions vs. short captions, etc.)


###### INTRODUCTION TO BLEU SCORE

As compared to other NLP tasks such as document classification, where evaluation of a model’s performance can be based on simple metrics such as accuracy or precision and recall, evaluation of generated captions is a more complex task.

A number of different metrics have been designed with the development of NLP to evalute generated text , however in this coursework in order to quantitatively assess the generated captions for both models, The Bilingual Evaluation Understudy metrix is used. The method is based on comparing the candidate text, the one generated by the model against the set of references, giving the score between 0 to 1. The higher the score is, the better generated candidate caption matches the reference text .
There are however several ways of calculating the score based on a weight attached to each n-gram. It is possible to calculate individual n-gram score, such as assessing only either 1-gram, 2,3 or 4-gram. For example, if we choose 1-gram evaluation, we would simply match individual words (Brownlee, 2017).
Another way of calculating the bleu score which is used in this report is cumulative n-gram score where each n-gram has assigned its own weight. I set my weights to be 0.45, 0.35, 0.10, 0.10 what gives higher weight to 1 and 2 grams and assigns little weight to 3 and 4-grams, while the default option assigns equal importance to each of the 1-4 n-grams. However, in image captioning the reference and candidate is being compared on a sentence level therefore finding 1 and 2 gram is more common than finding 3 or 4 grams. That approach allowed me to generate scores in a reasonable range.

The Bleu score was designed with the idea of document level comparison where in some sentences finding a 4-gram match between reference and candidate text was not rare. However, because we perform sentence level comparison smoothing function is employed as advised by Chen and Cherry (2014). The results of their study in which their assessed all seven smoothing methods show that method 7 provides the best human correlation therefore this method is chosen for this report.

###### COMPARISON BETWEEN THE MODELS

In order to compare Bleu Scores for both models, five captions where generated for images from the test set after training for five epochs (See Appendix 1). Due to a small sample size (5 captions) the bleu score was also calculated for the whole test set to validate the results.
We can see that the generated captions in our sample of 5 images obtained Bleu scores in range 0.28 – 0.66 (See Appendix 1). Although the highest score of 0.66 is still far from the perfect match of 1.0, we have to remember that it is difficult to obtain a score around 1.0 even using human written captions. This is because the Bleu score penaltizes any mismatches between reference and candidate. If the caption describes the image well but using different words or if the candidate has a different length than the true caption, the score gets reduced.

However, by looking at Appendix 1, there is a pattern of LSTM giving on average higher scores as compared to RNN, what can suggest that on average captions generated by LSTM perform slightly better than captions produced by RNN. There was only one caption in the sample (See Appendix 1, third caption) for which Bleu score obtained using RNN model was higher as compared to LSTM model. However, as already mentioned Bleu score penaltizes any mismatches of the length between candidate and reference text. The caption generated by LSTM is shorter than RNN’s caption what might result in a lower score.
Bleu score calculated over the whole test set confirmed the above results.
This time the Bleu score was calculated for each single image in the test set. The results were then aggregated and divided by the number of images what gave us the average Bleu score for each model.
  
The results are as follows: Bleu score over test set for LSTM is 0.48773541110981267 Bleu score over test set for RNN is 0.47576659190283077 Although the performance of LSTM is slightly higher than RNN, the difference is not significant, therefore further analysis is required.
  
###### PERFORMANCE ON LONG VS SHORT CAPTIONS
  
Two models were evaluated in terms of their performance on long vs short captions using Bleu score metric.
The Bleu score was calculated separately for long vs short captions for each model. Decision as to whether assign a generated caption to long or short category was based on the average length of reference captions. Because of the fact that we have 5 reference captions agaisnt 1 generated caption, the total number of tokens for all 5 reference captions was calculated and then divided by 5 what allowed me to obtain the average number of tokens for each set of reference caption. I then compared the number of tokens from generated caption against the average number of tokens for reference caption. Generated caption was classified as long if its number of tokens was greater than the average number of tokens for a corresponding set of reference caption.
  
The bleu score was calculated using the same weights as before including smoothing function with method 7.
The results are as follows:
  
RNN Bleu score for short caption: 0.4022034590856351 Bleu score for long caption: 0.35662719914252705

LSTM Bleu score for short caption: 0.4259603787052534 Bleu score for long caption: 0.38259930514032164

We can see that both models performed better in short captions as compared to long captions resulting in higher bleu scores for short captions.
The results confirm out previous findings from section 3.1.2 whereas bleu scores for both models were calculated over the whole test set. Again performance of LSTM is better than LSTM.
  
  
###### QUALITATIVE ASSESSMENT OF GENERATED CAPTIONS
  
  
Although the Bleu score gives an idea of how well candidate text matches reference text, it is based on simple assumption of comparing the two pieces of text based on matching n-grams and is unable to assess the semantic meaning of the caption what leads to low scores if the length of reference and candidate text differs or if they use different words even if the words are synonyms. Therefore further evaluation is required to analyse the captions produced by the two models.
The most reliable, although the most time consuming and expensive evaluation is a qualitative asssessmnet of generated captions performed by a human. Using a sample of five generated captions for both models (See Appendix 1) we can compare the quality of the captions generated by the models to check whether they are able to describe the most important features of the image.

First image displays a men in a water wearing a special suit/wetsuit. Both networks correctly identified a man on the picture although they failed to describe activity correctly. They both recognized water on the picture generating words which relate to water such us kayak, paddling a wave, wetsuit and surfing. However, if we had to choose which caption is better I would say that caption generated by LSTM model is of higher quality as it recognized a wetsuit correctly.
Higher quality of generated caption by LSTM is also visible while looking at the second image, which was described by RNN very innacurately. RNN’s model produced a caption using non frequent words which were replaced by ‘unk’ while LSTM managed to correctly identify activity and recognized a men on the picture with only minor mistake.
In terms of the third image we can say that again LSTM perfomed better. The caption does not fully describe the image however it describes image with some mistakes better than RNN. “Playing on a playground” is a more accurate description than “sitting on a bench” as generated by RNN.
The activity of fourth image was corectly described by both models with minor mistake made by RNN model which incorectly identified a bleu shirt on the picture.
Both captions generated for fifth image are related to the picture although they both describe the image with minor mistakes. Both models recognized a snowy field correctly however both failed to recognized activity and color of a jacket. However RNN model did not recognize that the picture describes an adult rathen than a young boy therefore we can say that LSTM again outperformed RNN model.

###### LOSS EVALUATION

The losses during training were collected and saved in a list. In image caption generation, we use cross entropy loss between the integer vectors of target ground true sequence and vector of generated caption (Zhu, Li, Liu, Guo, Fang, Peng, Niu, 2018). The graph below (see Figure 3) shows the losses for both models. We can see that they follow similar pattern slowy decreasing with RNN loss increasing after 4th spoch. That results in a final loss for LSTM of 2.3519 being sligthly smaller than the final loss for the RNN model of 2.5529. However, it is a common phenomenon is deep learning that loss fluctuates during training.
Figure 3: Graph of losses from LSTM and RNN models during training
The loss for both models is clearly converging however the final losses are still relatively high resulting in a number of captions being described incorrectly. However, we should remember that we trained the models only for 5 epochs and we did not change any paraments. Increasing training time and tuning parameters could give us lower loss and result in higher quality captions.
  
Additionally, attention layer could be used in order to focus on the most relevant parts of the image for determination of words by our network (Rister, Lawson, 2016).
However, by looking at captions generated during training (see Figure 1 and Figure 2) using the basic version of the model and training only for 5 epochs, the captions generated after each epoch are significantly improving. There is a significant semantic difference between captions generated after 1st and 2nd epoch (see Figure 1, top image). The LSTM model correctly identified that instead of men (as predicted after 1st epoch) there is a young boy. Moreover, two other important elements, a soccer ball and a red t-shirt was correctly generated by the caption. The caption generated after 5th epoch best describes an image as : ‘a boy in a red uniform is playing soccer’, leading to the highest Bleu score. We can see similar improvements being made using RNN model (See Figure 2, top image), however the quality of the final caption is slightly worse. On the other hand, caption generated for the second image (See Figure 1 and 2, bottom image) have many major mistakes. Although they both relate to the image there are some
major mistakes such as activity being recognized incorrectly. This can be explained by the fact that some images are difficult for the network to learn.




## References:

Jabeen, H., 2018. Stemming And Lemmatization In Python. [online] DataCamp Community. Available at: <https://www.datacamp.com/community/tutorials/stemming-lemmatization-python> [Accessed 23 April 2020]. 

SINGH, S., 2019. Remove Stopwords Using NLTK, Spacy And Gensim In Python. [online] Analytics Vidhya. Available at: <https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/> [Accessed 23 April 2020]. 

Shrestha, S., 2020. Image Captioning:. [online] Medium. Available at: <https://medium.com/@sushe012/image-captioning-dec977938d0e> [Accessed 22 April 2020]. 

Chen, B. and Cherry, C., 2014. A Systematic Comparison Of Smoothing Techniques For Sentence-Level BLEU. [online] Acl2014.org. Available at: <http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf> [Accessed 17 April 2020]. 

Mantecon, H., 2019. Deep Reinforcement Sequence Learning For Visual Captioning. [online] Pdfs.semanticscholar.org. Available at: <https://pdfs.semanticscholar.org/174d/1b037e62ab7ae5f73b597b025d5faf0fadb4.pdf> [Accessed 26 April 2020]. 

HOSSAIN, M., SOHEL, F., SHIRATUDDIN, M. and LAGA, H., 2018. A Comprehensive Survey Of Deep Learning For Image Captioning. [online] Arxiv.org. Available at: <https://arxiv.org/pdf/1810.04020.pdf> [Accessed 21 April 2020]. 

Rister, B. and Lawson, D., 2016. Image Captioning With Attention. [online] Cs231n.stanford.edu. Available at: <http://cs231n.stanford.edu/reports/2016/pdfs/362_Report.pdf> [Accessed 23 April 2020]. 

Zhu, C. and Li, L., 2018. Image Captioning With Word Gate and Adaptive Self-Critical Learning. [online]. Available at: < https://www.mdpi.com/2076-3417/8/6/909/htm> [Accessed 23 April 2020]. 

Batra, V., He, Y. and Vogiatzis, G., n.d. Neural Caption Generation For News Images. [online] Available at: <https://www.aclweb.org/anthology/L18-1273.pdf> [Accessed 26 April 2020]. 

Donnelly, C., 2020. Image Caption Generation With Recursive Neural Networks. [online] Cs224d.stanford.edu. Available at: <https://cs224d.stanford.edu/reports/cdonnelly.pdf> [Accessed 25 April 2020]. 

Cui, Y., Yang, G., Veit, A., Huang, X., Belongie, S. and Tech, C., 2018. Learning To Evaluate Image Captioning. [online] Arxiv.org. Available at: <https://arxiv.org/pdf/1806.06422.pdf> [Accessed 21 April 2020]. 

Kutuzov, A. and Kuzmenko, E., n.d. To Lemmatize Or Not To Lemmatize: How Word Normalisation Affects Elmo Performance In Word Sense Disambiguation. [online] Available at: <https://www.aclweb.org/anthology/W19-6203.pdf> [Accessed 25 April 2020].

Brownlee, J., 2019. A Gentle Introduction To Calculating The BLEU Score For Text In Python. [online] Machine Learning Mastery. Available at: <https://machinelearningmastery.com/calculate-bleu-score-for-text-python/> [Accessed 9 April 2020].
