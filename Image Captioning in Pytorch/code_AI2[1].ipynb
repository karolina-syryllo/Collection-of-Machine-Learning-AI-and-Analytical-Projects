{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a31sFIyrHaXl"
   },
   "source": [
    "# Image Caption Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81kdnnwJvTFx"
   },
   "source": [
    "## Text preparation \n",
    "\n",
    "Before moving onto the actual model, we first have to prepare the text.\n",
    "First step is to build a vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Ce3bfeFP_sH"
   },
   "source": [
    "Resnet- pre-trained network.\n",
    "\n",
    "CNN- to extract features using resnet.\n",
    "Then we pass it to the next part- Recursive Neural Network.\n",
    "\n",
    "5 different people describing and image. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ccewMuTpO56X",
    "outputId": "61d98ef7-44ec-400e-aa48-f47a80d56c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXpWOFqFOXcc"
   },
   "outputs": [],
   "source": [
    "# Mounted Drive if using Colab; otherwise, your local path\n",
    "root = \"drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8k_Dataset/\" \n",
    "caption_dir = root + \"captions/\"                     \n",
    "image_dir = root + \"images/\"                          \n",
    "\n",
    "\n",
    "token_file = \"Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9AkORttFoF_"
   },
   "source": [
    "A helper function to read in our ground truth text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHC0y7zaOXq8"
   },
   "outputs": [],
   "source": [
    "def read_lines(filepath):\n",
    "    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n",
    "    file = open(filepath, 'r')\n",
    "    lines = []\n",
    "\n",
    "    while True: \n",
    "        # Get next line from file until there's no more\n",
    "        line = file.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        if \"2258277193_586949ec62\" not in line.strip():     \n",
    "          lines.append(line.strip())\n",
    "    file.close() \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D86cJx2yv81K"
   },
   "source": [
    "You can read all the ground truth captions (5 per image), into memory as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m-snsM2XHuu"
   },
   "outputs": [],
   "source": [
    "lines = read_lines(caption_dir + token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IkK91ZuXNB2"
   },
   "outputs": [],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oksUJjLPwApA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # Intially, set both the IDs and words to empty dictionaries.\n",
    "        self.word2idx = {}    #two dictionaries because it's more efficient. if the word is not in dict it will add it\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # If the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # Increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEQtthpXwEoY"
   },
   "source": [
    "We extract all the words from ```lines```, and create a list of them in a variable ```words```, for example:\n",
    "\n",
    "```words = [\"a\", \"an\", \"the\", \"cat\"... ]```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "I9M3UWSAwAsM",
    "outputId": "b5baf0f8-41cf-4558-959e-d8e3e1c653ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way'], ['a', 'girl', 'going', 'into', 'a', 'wooden', 'building'], ['a', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse'], ['a', 'little', 'girl', 'climbing', 'the', 'stairs', 'to', 'her', 'playhouse'], ['a', 'little', 'girl', 'in', 'a', 'pink', 'dress', 'going', 'into', 'a', 'wooden', 'cabin'], ['a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting'], ['a', 'black', 'dog', 'and', 'a', 'tri-colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road'], ['a', 'black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street'], ['two', 'dogs', 'of', 'different', 'breeds', 'looking', 'at', 'each', 'other', 'on', 'the', 'road'], ['two', 'dogs', 'on', 'pavement', 'moving', 'toward', 'each', 'other']]\n",
      "40455\n",
      "3430\n"
     ]
    }
   ],
   "source": [
    "image_id_list=[]\n",
    "image_desc_list=[]\n",
    "\n",
    "for line in lines:\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc=tokens[0], tokens[1:]\n",
    "    image_id_list.append(image_id)\n",
    "    image_desc_list.append(image_desc)\n",
    "\n",
    "words=[[char.lower() for char in val] for val in image_desc_list]\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "symbols=list(punctuation)\n",
    "\n",
    "words = [[c for c in char if c not in symbols] for char in words]\n",
    "print(words[:10])\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "# count each element in a list (like after unnesting them))\n",
    "freq=Counter([x for a in words for x in a])\n",
    "\n",
    "\n",
    "# delete elements in sublist is they appear less than 3 times in a whole text\n",
    "words_frequent = [[ele for ele in char if freq[ele] > 3] for char in words]\n",
    "print(len(words_frequent))\n",
    "\n",
    "\n",
    "# create a whole big unnested list\n",
    "words_list = [ item for elem in words_frequent for item in elem]\n",
    "\n",
    "#check how many distinctive values are there\n",
    "print(len(set(words_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHBMe-ATwLIQ"
   },
   "source": [
    "We build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctwErx_ZwAzB"
   },
   "outputs": [],
   "source": [
    "# Create a vocab instance\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# Add the token words first\n",
    "vocab.add_word('<pad>')\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "vocab.add_word('<unk>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzEYIvJ-GA_G"
   },
   "source": [
    "Add the rest of the words from the parsed captions:\n",
    "\n",
    "``` vocab.add_word('new_word')```\n",
    "\n",
    "We don't add words that appear three times or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zj99JT2XwA4-",
    "outputId": "8d3398fa-bb19-45bf-826c-5f8423f9115d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3434\n"
     ]
    }
   ],
   "source": [
    "for word in words_list:\n",
    "  vocab.add_word(word)\n",
    "\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChA2jcHxX0Eg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FB30f4wYwSvg"
   },
   "source": [
    "## Dataset and loaders for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raEOHrpnbbKY"
   },
   "source": [
    "Keeping the same order, we concatenate all the cleaned words from each caption into a string again, and add them all to a list of strings ```cleaned_captions```. We then store all the image ids in a list ```image_ids```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "UGGnaDIRbZUs",
    "outputId": "3d9e4c5e-1b83-4b40-9fad-9e2f7f759680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a child in a pink dress is climbing up a set of stairs in an entry way', 'a girl going into a wooden building', 'a little girl climbing into a wooden playhouse', 'a little girl climbing the stairs to her playhouse', 'a little girl in a pink dress going into a wooden cabin']\n",
      "['1000268201_693b08cb0e', '1000268201_693b08cb0e', '1000268201_693b08cb0e', '1000268201_693b08cb0e', '1000268201_693b08cb0e', '1001773457_577c3a7d70', '1001773457_577c3a7d70', '1001773457_577c3a7d70', '1001773457_577c3a7d70', '1001773457_577c3a7d70']\n"
     ]
    }
   ],
   "source": [
    "#join each element of a list into a string\n",
    "cleaned_captions=[\" \".join(char) for char in words]\n",
    "print(cleaned_captions[:5])\n",
    "\n",
    "image_ids=image_id_list\n",
    "\n",
    "image_ids=[x.split(\".\")[0] for x in image_ids]\n",
    "print(image_ids[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_FbII1VwVSg"
   },
   "source": [
    "The dataframe for the image paths and captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "TYQz4T3mwA2o",
    "outputId": "d41a864a-eaab-4400-ff0e-4efebfad0c04"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [image_id, path, caption]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'image_id': image_ids,\n",
    "    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n",
    "    'caption': cleaned_captions\n",
    "}\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])\n",
    "\n",
    "data_df['image_id'] == \"2258277193_586949ec62\"\n",
    "\n",
    "data_df.loc[data_df['image_id'] == \"2258277193_586949ec62\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "POB7UiJLwYsf",
    "outputId": "a2b35c5e-0b05-49e4-8a3b-74a8e76bd896"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a girl going into a wooden building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a little girl climbing into a wooden playhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1001773457_577c3a7d70</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a black dog and a spotted dog are fighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001773457_577c3a7d70</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a black dog and a tri-colored dog playing with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1001773457_577c3a7d70</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>a black dog and a white dog with brown spots a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1001773457_577c3a7d70</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>two dogs of different breeds looking at each o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1001773457_577c3a7d70</td>\n",
       "      <td>drive/My Drive/Colab Notebooks/ai_cw_2/Flickr8...</td>\n",
       "      <td>two dogs on pavement moving toward each other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image_id  ...                                            caption\n",
       "0  1000268201_693b08cb0e  ...  a child in a pink dress is climbing up a set o...\n",
       "1  1000268201_693b08cb0e  ...                a girl going into a wooden building\n",
       "2  1000268201_693b08cb0e  ...     a little girl climbing into a wooden playhouse\n",
       "3  1000268201_693b08cb0e  ...  a little girl climbing the stairs to her playh...\n",
       "4  1000268201_693b08cb0e  ...  a little girl in a pink dress going into a woo...\n",
       "5  1001773457_577c3a7d70  ...         a black dog and a spotted dog are fighting\n",
       "6  1001773457_577c3a7d70  ...  a black dog and a tri-colored dog playing with...\n",
       "7  1001773457_577c3a7d70  ...  a black dog and a white dog with brown spots a...\n",
       "8  1001773457_577c3a7d70  ...  two dogs of different breeds looking at each o...\n",
       "9  1001773457_577c3a7d70  ...      two dogs on pavement moving toward each other\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNLQ0K-_weJy"
   },
   "source": [
    "This is the Flickr8k class for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqf2_F6YwakD"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "from nltk import tokenize\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Flickr8k(Dataset):\n",
    "    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n",
    "    \n",
    "    def __init__(self, df, vocab, transform=None):\n",
    "\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        vocab = self.vocab\n",
    "\n",
    "        caption = self.df['caption'][index]\n",
    "        img_id = self.df['image_id'][index]\n",
    "        path = self.df['path'][index]\n",
    "\n",
    "        image = Image.open(open(path, 'rb'))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = caption.split()\n",
    "        caption = []\n",
    "        # Build the Tensor version of the caption, with token words\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vkld_4CwkPO"
   },
   "source": [
    "We need to overwrite the default PyTorch ```collate_fn()``` because our ground truth captions are sequential data of varying lengths. The default ```collate_fn()``` does not support merging the captions with padding.\n",
    "\n",
    "You can read more about it here: https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5YmKr9ewkqO"
   },
   "outputs": [],
   "source": [
    "def caption_collate_fn(data):\n",
    "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length from longest to shortest.\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6VDx2O5FSiM"
   },
   "source": [
    "Now we define the data transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpRbVk6BFTGD"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgS9OpZ7FaAj"
   },
   "source": [
    "Initialising the datasets. The only twist is that every image has 5 ground truth captions, so each image appears five times in the dataframe. We don't want an image to appear in more than one set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnTvR684GGVV"
   },
   "outputs": [],
   "source": [
    "unit_size = 5\n",
    "import torch\n",
    "train_split = 0.95 # Defines the ratio of train/test data.\n",
    "\n",
    "# We didn't shuffle the dataframe yet so this works\n",
    "train_size = unit_size * round(len(data_df)*train_split / unit_size)\n",
    "\n",
    "dataset_train = Flickr8k(\n",
    "    df=data_df[:train_size].reset_index(drop=True),\n",
    "    vocab=vocab,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "dataset_test = Flickr8k(\n",
    "    df=data_df[(train_size):].reset_index(drop=True),\n",
    "    vocab=vocab,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWuWg72dGOq9"
   },
   "source": [
    "Write the dataloaders ```train_loader``` and ```test_loader``` - explicitly replacing the collate_fn:\n",
    "\n",
    "```train_loader = torch.utils.data.DataLoader(\n",
    "  ...,\n",
    "  collate_fn=caption_collate_fn\n",
    ")```\n",
    "\n",
    "Set train batch size to 128 and be sure to set ```shuffle=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkNrIRbXGLFG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=5, \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXlf8lt5TF0N"
   },
   "source": [
    "## Encoder and decoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ls8lyXA2GTC0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True) # Pre-trained on ImageNet by default\n",
    "        layers = list(resnet.children())[:-1]      # Keep all layers except the last one\n",
    "        # Unpack the layers and create a new Sequential\n",
    "        self.resnet = nn.Sequential(*layers)\n",
    "        \n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # What is an embedding layer?\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        #self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed) # Replace with self.rnn when using RNN\n",
    "        #hiddens, _ = self.rnn(packed) # Replace with self.rnn when using RNN\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seq_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size) # replaced with rnn\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C9-qtkkMTrtB",
    "outputId": "331a8eff-4fc7-4a96-ea3e-1abe8451d9c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 150,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhecFOMRUgpe"
   },
   "source": [
    "Set training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fd2-IX2Uer3"
   },
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "log_step = 10\n",
    "save_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlIwF6P8UgB4"
   },
   "source": [
    "Initialize the models and set the learning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxwDUlR2Uy7t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build the models\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimisation will be on the parameters of BOTH the enocder and decoder,\n",
    "# but excluding the ResNet parameters, only the new added layers.\n",
    "params = list(\n",
    "    decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trxTtzy_qXZY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUmSb2MHEZw3"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uS4oN21vNKu7"
   },
   "source": [
    "A set of functions to generate sample captions displayed with images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUiltjxODi7N"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import argparse\n",
    "import pickle \n",
    "import os\n",
    "from torchvision import transforms \n",
    "from PIL import Image\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#!pip install q nltk==3.5\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "#print('The nltk version is {}.'.format(nltk.__version__))\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def imshow(inp, figsize=None, title=None):\n",
    "    if figsize != None:\n",
    "        plt.figure(figsize=figsize)\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  \n",
    "    plt.show()\n",
    "\n",
    "def get_sentence(caption_tensor): \n",
    "    # Convert word_ids to words\n",
    "    sampled_caption = []\n",
    "    for word_id in caption_tensor:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption[1:-1])\n",
    "    return sentence\n",
    "\n",
    "def transform():\n",
    "  transformed_image = transforms.Compose([ \n",
    "  transforms.Resize(224),\n",
    "  transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
    "                        (0.229, 0.224, 0.225))])\n",
    "  return transformed_image\n",
    "\n",
    "\n",
    "def find_captions(caption_tensors):\n",
    "    reference_captions = []\n",
    "\n",
    "    for i in range(5):\n",
    "      reference_captions.append(get_sentence(caption_tensors[i]))\n",
    "\n",
    "    return reference_captions\n",
    "\n",
    "def get_sample_2():\n",
    "    score_list=[]\n",
    "    test_iterator = iter(test_loader)\n",
    "    encoder.eval()\n",
    "    for i in range(2):\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "\n",
    "      images = images.to(device)\n",
    "      captions = captions.to(device)\n",
    "      features = encoder(images)\n",
    "\n",
    "      sample_ids = decoder.sample(features)\n",
    "      sample_ids=sample_ids.cpu().numpy()\n",
    "      generated_sentence=get_sentence(sample_ids[0])\n",
    "      print(generated_sentence)\n",
    "      \n",
    "      true_ids = captions.cpu().numpy()\n",
    "      sentence_true = find_captions(true_ids)\n",
    "\n",
    "      #split the string into words to calculate bleu score for both true caption and generated caption\n",
    "      # TRUE CAPTION SPLIT\n",
    "      sentence_true_words=[ [i] for i in sentence_true]\n",
    "      sentence_true_words=[ string[0].split() for string  in sentence_true_words]\n",
    "      \n",
    "      #GENERATED CAPTION SPLIT\n",
    "      generated_sentence_words=list(generated_sentence.split())\n",
    "   \n",
    "      #calculate bleu score\n",
    "      score = sentence_bleu(sentence_true_words, generated_sentence_words, weights=(0.45, 0.35, 0.10, 0.10), smoothing_function=SmoothingFunction().method7)\n",
    "      print(\"Bleu score for generated caption: {}\".format(score))\n",
    "\n",
    "      out = make_grid(images[0])\n",
    "      out=out.cpu()\n",
    "      imshow(out, figsize=(10,6), title='Target: %s\\nPrediction: %s' % (sentence_true, generated_sentence))\n",
    "\n",
    "      score_list.append(score)\n",
    "      \n",
    "    encoder.train()\n",
    "    return score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HQ4cVOp0p74"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYgiayFGqk9Z"
   },
   "source": [
    "## TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PdrTSRuFWib"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "#uncomment when running RNN\n",
    "# bleu_scoreRNN_list =[]\n",
    "# loss_rnn=[]\n",
    "\n",
    "\n",
    "bleu_scoreLSTM_list=[]\n",
    "loss_lstm=[]\n",
    "\n",
    "file_name = \"checkpoint.pth\"\n",
    "\n",
    "print(\"__________________Before training__________________\")\n",
    "get_sample_2()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Packed as well as we'll compare to the decoder outputs\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Zero gradients for both networks\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % log_step == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch, num_epochs, i, total_step, loss.item()))\n",
    "\n",
    "    #     if (i+1) % save_step == 0:\n",
    "    #       torch.save(encoder.state_dict(), file_name + '-RNNencoder.ckpt')\n",
    "    #       torch.save(decoder.state_dict(), file_name + '-RNNdecoder.ckpt')\n",
    "    # print('Generated captions after epoch: {}'.format(epoch+1))\n",
    "    x=get_sample_2()\n",
    "\n",
    "\n",
    "    # bleu_scoreRNN_list.append(x)\n",
    "    # loss_rnn.append(loss.item(x))\n",
    "\n",
    "\n",
    "    bleu_scoreLSTM_list.append(x)\n",
    "    loss_lstm.append(loss.item(x))\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1cuWKmwxcMx"
   },
   "outputs": [],
   "source": [
    "score1RNN=[el[0] for el in bleu_scoreRNN_list]\n",
    "score2RNN=[el[1] for el in bleu_scoreRNN_list]  \n",
    "\n",
    "\n",
    "score1LSTM=[el[0] for el in bleu_scoreLSTM_list]\n",
    "score2LSTM=[el[1] for el in bleu_scoreLSTM_list]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jv2R-DHW61p4"
   },
   "outputs": [],
   "source": [
    "# bleu score for first image rnn [0.5153939645243383, 0.5782537389840694, 0.5105593001127932, 0.45272728894361675, 0.6022782201270712]\n",
    "# bleu score for second image rnn [0.5019846346860442, 0.42427489304239147, 0.6442534123707138, 0.4723402003873197, 0.4465502347955529]\n",
    "# loss rnn [2.8743698596954346, 2.7102692127227783, 2.702296733856201, 2.4979448318481445, 2.552945137023926]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "LXjroCPgKvDR",
    "outputId": "f4281b34-f4cb-4e9b-abfb-6f8178959148"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def plot_loss(loss_lstm, loss_rnn):\n",
    "    x = np.arange(0, 5)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, loss_lstm, \"r\", linewidth=1, label=\"loss LSTM\")\n",
    "\n",
    "    plt.plot(x, loss_rnn, \"b\", linewidth=1, label=\"loss RNN\")\n",
    "\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"Loss of LSTM and RNN\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(loss_lstm, loss_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzh_vNwbrEF1"
   },
   "source": [
    "##Model's evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LztSIRm-r1KE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# GENERATE SAMPLE OF 5 IMAGES ALONG WITH CAPTIONS AND BLEU SCORES\n",
    "\n",
    "def get_sample_5():\n",
    "    generated_list=[]\n",
    "    test_iterator = iter(test_loader)\n",
    "    encoder.eval()\n",
    "    for i in range(5):\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images, captions, lengths = next(test_iterator)\n",
    "      images = images.to(device)\n",
    "      captions = captions.to(device)\n",
    "      features = encoder(images)\n",
    "      print()\n",
    "\n",
    "      sample_ids = decoder.sample(features)\n",
    "      sample_ids=sample_ids.cpu().numpy()\n",
    "      generated_sentence=get_sentence(sample_ids[0])\n",
    "      \n",
    "      true_ids = captions.cpu().numpy()\n",
    "      sentence_true = find_captions(true_ids)\n",
    "\n",
    "      #split the string into words to calculate bleu score for both true caption and generated caption\n",
    "      # TRUE CAPTION SPLIT\n",
    "      sentence_true_words=[ [i] for i in sentence_true]\n",
    "      sentence_true_words=[ string[0].split() for string  in sentence_true_words]\n",
    "      #print(sentence_true_words)\n",
    "      \n",
    "      #GENERATED CAPTION SPLIT\n",
    "      generated_sentence_words=list(generated_sentence.split())\n",
    "   \n",
    "      #calculate bleu score\n",
    "      score = sentence_bleu(sentence_true_words, generated_sentence_words, weights=(0.45, 0.35, 0.1, 0.1), smoothing_function=SmoothingFunction().method7) \n",
    "      print(\"Bleu score for generated caption: {}\".format(score))\n",
    "\n",
    "      #display image\n",
    "      out = make_grid(images[0])\n",
    "      out=out.cpu()\n",
    "      imshow(out, figsize=(10,6), title='Target: %s\\nPrediction: %s' % (sentence_true, generated_sentence))\n",
    "      generated_list.append(generated_sentence)\n",
    "\n",
    "    encoder.train()\n",
    "    return generated_list\n",
    "\n",
    "\n",
    "get_sample_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "edfV8XuidR-W",
    "outputId": "13be4c03-6905-47bc-85d3-7ba26dfe5255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48773541110981267\n"
     ]
    }
   ],
   "source": [
    "#calculate Bleu score over the whole test set\n",
    "\n",
    "def bleu_score_test():\n",
    "  score_total=0\n",
    "  for i, (images, captions, lengths) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    features = encoder(images)\n",
    "    encoder.eval()\n",
    "\n",
    "    sample_ids = decoder.sample(features)\n",
    "    sample_ids=sample_ids.cpu().numpy()\n",
    "    generated_sentence=get_sentence(sample_ids[0])\n",
    "    \n",
    "    true_ids = captions.cpu().numpy()\n",
    "    sentence_true = find_captions(true_ids)\n",
    "\n",
    "    sentence_true_words=[ [i] for i in sentence_true]\n",
    "    sentence_true_words=[ string[0].split() for string  in sentence_true_words]\n",
    "\n",
    "    generated_sentence_words=list(generated_sentence.split())\n",
    "  \n",
    "    #calculate bleu score\n",
    "    score = sentence_bleu(sentence_true_words, generated_sentence_words, weights=(0.45, 0.35, 0.1, 0.1), smoothing_function=SmoothingFunction().method7) \n",
    "    #print(\"Bleu score for generated caption: {}\".format(score))\n",
    "\n",
    "    score_total+=score\n",
    "  print(score_total/len(test_loader))\n",
    "  \n",
    "  \n",
    "    \n",
    "\n",
    "bleu_score_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_iQQ7qCpGMM0",
    "outputId": "a7579ef2-7336-49ab-8d13-abafbf37e391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of short captions:233\n",
      "Number of long captions:172\n",
      "Bleu score for short caption: 0.4259603787052534\n",
      "Bleu score for long caption: 0.38259930514032164\n"
     ]
    }
   ],
   "source": [
    "# CALCULATES PERFORMANCE ON SHORT VS LONG CAPTIONS\n",
    "\n",
    "from nltk import flatten\n",
    "\n",
    "def long_short():\n",
    "  long_caption_generated=[]\n",
    "  long_caption_ref=[]\n",
    "  short_caption_generated=[]\n",
    "  short_caption_ref=[]\n",
    "  for i, (images, captions, lengths) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    features = encoder(images)\n",
    "    encoder.eval()\n",
    "\n",
    "    sample_ids = decoder.sample(features)\n",
    "    sample_ids=sample_ids.cpu().numpy()\n",
    "    generated_sentence=get_sentence(sample_ids[0])\n",
    "    \n",
    "    true_ids = captions.cpu().numpy()\n",
    "    sentence_true = find_captions(true_ids)\n",
    "\n",
    "    sentence_true_words=[ [i] for i in sentence_true]\n",
    "    sentence_true_words=[ string[0].split() for string  in sentence_true_words]\n",
    "\n",
    "    generated_sentence_words=list(generated_sentence.split())\n",
    "\n",
    "    gen_len=len(generated_sentence_words)\n",
    "    true_len=len(flatten(sentence_true_words))/5\n",
    "\n",
    "    if gen_len > true_len :\n",
    "      long_caption_generated.append(generated_sentence_words)\n",
    "      long_caption_ref.append(sentence_true_words)\n",
    "    else:\n",
    "      short_caption_generated.append(generated_sentence_words)\n",
    "      short_caption_ref.append(sentence_true_words)\n",
    "\n",
    "\n",
    "  bleu_score_long=corpus_bleu(long_caption_ref, long_caption_generated, weights=(0.45, 0.35, 0.1, 0.1), smoothing_function=SmoothingFunction().method7)\n",
    "  bleu_score_short=corpus_bleu(short_caption_ref, short_caption_generated,weights=(0.45, 0.35, 0.1, 0.1), smoothing_function=SmoothingFunction().method7)\n",
    "    \n",
    "  print(\"Number of short captions:{}\".format(len(short_caption_generated)))\n",
    "  print(\"Number of long captions:{}\".format(len(long_caption_generated)))\n",
    "\n",
    "    \n",
    "  print(\"Bleu score for short caption: {}\".format(bleu_score_short))\n",
    "  print(\"Bleu score for long caption: {}\".format(bleu_score_long))\n",
    "\n",
    "long_short()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "COMP5623_CW2_Starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
